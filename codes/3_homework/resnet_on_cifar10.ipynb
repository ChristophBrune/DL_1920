{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"resnet_on_cifar10.ipynb","provenance":[{"file_id":"1AswAne0soFX43THh56ZlZa7VQfgRLIGo","timestamp":1576010367797}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"XsdKToFor69U","colab_type":"text"},"source":["# Homework 3, exercise 2 - Residual Neural Network on CIFAR10\n","\n","In this exercise we implement a (slightly modified) ResNet as introduced in [this paper](https://arxiv.org/pdf/1512.03385.pdf)."]},{"cell_type":"code","metadata":{"id":"1VdY58D3KMZO","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","import time"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sRuR6CcbsW8_","colab_type":"text"},"source":["For this exercise it is recommended to use the GPU!"]},{"cell_type":"code","metadata":{"id":"rhZQhrlxKSTK","colab_type":"code","outputId":"080851ee-4233-4e39-e5ab-ecde4d88530a","executionInfo":{"status":"ok","timestamp":1576003485265,"user_tz":-60,"elapsed":978,"user":{"displayName":"Ron van Bree","photoUrl":"","userId":"10574495138637938052"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["\n","use_cuda = True\n","\n","if use_cuda and torch.cuda.is_available():\n","  device = torch.device('cuda')\n","else:\n","  device = torch.device('cpu')\n","\n","device"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"pwJz3i37UXsZ","colab_type":"text"},"source":["### Load the CIFAR10 dataset"]},{"cell_type":"code","metadata":{"id":"e1WVamZiKSXR","colab_type":"code","outputId":"3ad28be0-c0d3-48f8-de13-5bbdc49dd617","executionInfo":{"status":"ok","timestamp":1576005182727,"user_tz":-60,"elapsed":2310,"user":{"displayName":"Ron van Bree","photoUrl":"","userId":"10574495138637938052"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["import torchvision\n","import torchvision.transforms as transforms\n","\n","transform_train = transforms.Compose([\n","    transforms.RandomCrop(32, padding=4),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","])\n","\n","transform_test = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","])\n","\n","trainset = torchvision.datasets.CIFAR10(root='./data_cifar', train=True,\n","                                        download=True, transform=transform_train)\n","\n","testset = torchvision.datasets.CIFAR10(root='./data_cifar', train=False,\n","                                       download=True, transform=transform_test)\n","\n","batch_size = 128\n","\n","c, w, h = 3, 32, 32\n","\n","trainloader = torch.utils.data.DataLoader(trainset,\n","                                          batch_size=batch_size,\n","                                          shuffle=True)\n","\n","testloader = torch.utils.data.DataLoader(testset,\n","                                         batch_size=batch_size,\n","                                         shuffle=True)\n","\n","classes = ('plane', 'car', 'bird', 'cat',\n","           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Files already downloaded and verified\n","Files already downloaded and verified\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"hfpdVQRbUg5p","colab_type":"text"},"source":["## Exercise - Implement a Residual Block\n","\n","Residual neural networks mainly consist of components called Residual Blocks. One residual block can be expressed as **y** = *F*(**x**) + **x** where **x** and **y** are the input and output of the block, respectively. So the input **x** is added to the result of *F*(**x**) using a *skip connection*. In this exercise, *F* consists of:\n","* a convolutional layer with `in_channels` input channels, `hidden_channels` output channels, a kernel size of (3, 3), a stride of 1, padding of 1 and no bias parameter.\n","* a batch normalisation layer \n","* ReLU activation\n","* a convolutional layer with `hidden_channels` input channels, `out_channels` output channels, a kernel size of (3, 3), a stride of 1, padding of 1 and no bias parameter.\n","* a batch normalisation layer\n","\n","After this the `skip_connection` is applied. If the dimensions of *F*(**x**) and **x** don't match an extra linear projection is applied to **x** so the dimensions do match. This has already been implemented for you. You only need to call it at the right place. \n","Finally, a ReLU activation is applied on the output **y**\n"]},{"cell_type":"code","metadata":{"id":"HK1qpjYwUFqh","colab_type":"code","colab":{}},"source":["class ResidualBlock(nn.Module):\n","\n","  def __init__(self, in_channels, hidden_channels, out_channels):\n","    super().__init__()\n","\n","    # Complete the code here!\n","\n","\n","    if in_channels != out_channels:  # F(x) and x dimensions do not match! Define a projection for input x\n","      self.skip_connection = nn.Sequential(\n","          nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False),\n","          nn.BatchNorm2d(out_channels)\n","      )\n","    else:\n","      self.skip_connection = lambda x: x  # The dimensions already match! No need to do a projection on x\n","\n","  def forward(self, x):\n","    pass  # Complete the code here!\n","\n","\n","  "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T1Y87D77cYX8","colab_type":"text"},"source":["## Exercise - Implement a Residual Neural Network\n","Now you can use the previously defined Residual Block to create your ResNet.\n","\n","The network consists of:\n","* a convolutional layer with `in_channels` input channels, 64 output channels, a stride of 1, padding of 1 and no bias parameter,\n","* a batch normalisation layer\n","* ReLU activation\n","* a max pooling layer with kernel size (3, 3), a stride of 2 and padding of 1,\n","* eight residual blocks, with (64, 64, 128, 128, 256, 256, 512, 512) channels, respectively (see code below) \n","* an average pooling layer over all feature maps (already present)\n","* a dense layer to form the output distribution (already present)"]},{"cell_type":"code","metadata":{"id":"0qVgN9lPKSeC","colab_type":"code","colab":{}},"source":["class ResNet(nn.Module):\n","\n","  def __init__(self, in_channels, out_size):\n","    super().__init__()\n","\n","    # Complete the code here!\n","\n","\n","    self.res_blocks = nn.ModuleList(\n","        [\n","         ResidualBlock(64, 64, 64),\n","         ResidualBlock(64, 64, 64),\n","         \n","         ResidualBlock(64, 128, 128),\n","         ResidualBlock(128, 128, 128),\n","         \n","         ResidualBlock(128, 256, 256),\n","         ResidualBlock(256, 256, 256),\n","\n","         ResidualBlock(256, 512, 512),\n","         ResidualBlock(512, 512, 512),\n","        ]\n","    )\n","\n","    self.dense_layer = nn.Linear(512, out_size)\n","    \n","    for module in self.modules():\n","      if isinstance(module, nn.Conv2d):\n","          nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')\n","\n","  def forward(self, x):  \n","\n","    # Complete the code here!\n","    # Add everything that needs to be done before the average pooling\n","\n","\n","    x = F.avg_pool2d(x, x.shape[2:])\n","    \n","    x = x.view(x.size(0), -1)\n","    x = self.dense_layer(x)\n","\n","    return x\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DZ9ny4USgNAu","colab_type":"text"},"source":["### Initialize the network, Loss function and Optimizer"]},{"cell_type":"code","metadata":{"id":"FIofWmkrT6Oh","colab_type":"code","colab":{}},"source":["net = ResNet(c, len(classes)).to(device)\n","\n","criterion = nn.CrossEntropyLoss()\n","\n","optimizer = torch.optim.Adam(net.parameters(), lr=0.001)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ojw0pS0dgZHX","colab_type":"text"},"source":["## Exercise - Train/evaluate the network\n","Train the network you built using the code below. Add the following answers in your report:\n","* What test accuracy were you able to get?\n","* How many layers does your network have? (counting only convolutional and dense layers)\n","* Why do the skip connections help for training deep neural networks?"]},{"cell_type":"code","metadata":{"id":"IcG_bfjoT7Dx","colab_type":"code","outputId":"a5280aa9-5f1c-4867-b0e8-8444b5795ee5","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["start=time.time()\n","\n","for epoch in range(0,200):\n","\n","  net.train()  # Put the network in train mode\n","  for i, (x_batch, y_batch) in enumerate(trainloader):\n","    x_batch, y_batch = x_batch.to(device), y_batch.to(device)  # Move the data to the device that is used\n","    \n","    optimizer.zero_grad()  # Set all currenly stored gradients to zero \n","\n","    y_pred = net(x_batch)\n","\n","    loss = criterion(y_pred, y_batch)\n","\n","    loss.backward()\n","\n","    optimizer.step()\n","\n","    # Compute relevant metrics\n","    \n","    y_pred_max = torch.argmax(y_pred, dim=1)  # Get the labels with highest output probability\n","\n","    correct = torch.sum(torch.eq(y_pred_max, y_batch)).item()  # Count how many are equal to the true labels\n","\n","    elapsed = time.time() - start  # Keep track of how much time has elapsed\n","\n","    # Show progress every 20 batches \n","    if not i % 20:\n","      print(f'epoch: {epoch}, time: {elapsed:.3f}s, loss: {loss.item():.3f}, train accuracy: {correct / batch_size:.3f}')\n","    \n","    correct_total = 0\n","\n","  net.eval()  # Put the network in eval mode\n","  for i, (x_batch, y_batch) in enumerate(testloader):\n","    x_batch, y_batch = x_batch.to(device), y_batch.to(device)  # Move the data to the device that is used\n","\n","    y_pred = net(x_batch)\n","    y_pred_max = torch.argmax(y_pred, dim=1)\n","\n","    correct_total += torch.sum(torch.eq(y_pred_max, y_batch)).item()\n","\n","  print(f'Accuracy on the test set: {correct_total / len(testset):.3f}')\n","\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["epoch: 0, time: 0.187s, loss: 2.434, train accuracy: 0.164\n","epoch: 0, time: 3.413s, loss: 1.732, train accuracy: 0.312\n","epoch: 0, time: 6.653s, loss: 1.546, train accuracy: 0.375\n","epoch: 0, time: 9.881s, loss: 1.650, train accuracy: 0.352\n","epoch: 0, time: 13.106s, loss: 1.629, train accuracy: 0.391\n","epoch: 0, time: 16.346s, loss: 1.451, train accuracy: 0.477\n","epoch: 0, time: 19.565s, loss: 1.556, train accuracy: 0.398\n","epoch: 0, time: 22.777s, loss: 1.478, train accuracy: 0.430\n","epoch: 0, time: 26.013s, loss: 1.261, train accuracy: 0.539\n","epoch: 0, time: 29.234s, loss: 1.139, train accuracy: 0.555\n","epoch: 0, time: 32.453s, loss: 1.194, train accuracy: 0.547\n","epoch: 0, time: 35.680s, loss: 1.164, train accuracy: 0.602\n","epoch: 0, time: 38.937s, loss: 1.201, train accuracy: 0.555\n","epoch: 0, time: 42.174s, loss: 1.315, train accuracy: 0.492\n","epoch: 0, time: 45.423s, loss: 1.143, train accuracy: 0.594\n","epoch: 0, time: 48.673s, loss: 1.298, train accuracy: 0.602\n","epoch: 0, time: 51.912s, loss: 1.189, train accuracy: 0.625\n","epoch: 0, time: 55.138s, loss: 1.078, train accuracy: 0.578\n","epoch: 0, time: 58.352s, loss: 1.146, train accuracy: 0.539\n","epoch: 0, time: 61.574s, loss: 1.246, train accuracy: 0.562\n","Accuracy on the test set: 0.564\n","epoch: 1, time: 68.676s, loss: 1.170, train accuracy: 0.586\n","epoch: 1, time: 71.903s, loss: 1.040, train accuracy: 0.594\n","epoch: 1, time: 75.149s, loss: 1.117, train accuracy: 0.578\n","epoch: 1, time: 78.367s, loss: 1.001, train accuracy: 0.672\n","epoch: 1, time: 81.592s, loss: 0.848, train accuracy: 0.672\n","epoch: 1, time: 84.829s, loss: 0.934, train accuracy: 0.672\n","epoch: 1, time: 88.059s, loss: 1.116, train accuracy: 0.594\n","epoch: 1, time: 91.274s, loss: 0.989, train accuracy: 0.633\n","epoch: 1, time: 94.501s, loss: 1.013, train accuracy: 0.633\n","epoch: 1, time: 97.740s, loss: 0.900, train accuracy: 0.664\n","epoch: 1, time: 100.977s, loss: 0.967, train accuracy: 0.656\n","epoch: 1, time: 104.209s, loss: 0.975, train accuracy: 0.648\n","epoch: 1, time: 107.445s, loss: 0.924, train accuracy: 0.719\n","epoch: 1, time: 110.700s, loss: 1.024, train accuracy: 0.648\n","epoch: 1, time: 113.932s, loss: 1.053, train accuracy: 0.688\n","epoch: 1, time: 117.169s, loss: 0.887, train accuracy: 0.711\n","epoch: 1, time: 120.411s, loss: 0.744, train accuracy: 0.727\n","epoch: 1, time: 123.636s, loss: 0.953, train accuracy: 0.664\n","epoch: 1, time: 126.848s, loss: 1.052, train accuracy: 0.594\n","epoch: 1, time: 130.061s, loss: 0.935, train accuracy: 0.641\n","Accuracy on the test set: 0.561\n","epoch: 2, time: 137.176s, loss: 0.808, train accuracy: 0.688\n","epoch: 2, time: 140.404s, loss: 0.784, train accuracy: 0.734\n","epoch: 2, time: 143.649s, loss: 0.908, train accuracy: 0.656\n","epoch: 2, time: 146.878s, loss: 0.646, train accuracy: 0.789\n","epoch: 2, time: 150.091s, loss: 0.855, train accuracy: 0.695\n","epoch: 2, time: 153.316s, loss: 0.743, train accuracy: 0.734\n","epoch: 2, time: 156.541s, loss: 0.703, train accuracy: 0.758\n","epoch: 2, time: 159.777s, loss: 0.916, train accuracy: 0.656\n","epoch: 2, time: 163.000s, loss: 0.780, train accuracy: 0.758\n","epoch: 2, time: 166.240s, loss: 0.747, train accuracy: 0.711\n","epoch: 2, time: 169.476s, loss: 0.727, train accuracy: 0.727\n","epoch: 2, time: 172.721s, loss: 0.849, train accuracy: 0.727\n","epoch: 2, time: 175.955s, loss: 0.814, train accuracy: 0.695\n","epoch: 2, time: 179.183s, loss: 0.679, train accuracy: 0.773\n","epoch: 2, time: 182.405s, loss: 0.796, train accuracy: 0.758\n","epoch: 2, time: 185.628s, loss: 0.676, train accuracy: 0.766\n","epoch: 2, time: 188.853s, loss: 0.595, train accuracy: 0.797\n","epoch: 2, time: 192.079s, loss: 0.761, train accuracy: 0.750\n","epoch: 2, time: 195.288s, loss: 0.815, train accuracy: 0.742\n","epoch: 2, time: 198.536s, loss: 0.842, train accuracy: 0.750\n","Accuracy on the test set: 0.679\n","epoch: 3, time: 205.651s, loss: 0.805, train accuracy: 0.672\n","epoch: 3, time: 208.872s, loss: 0.521, train accuracy: 0.812\n","epoch: 3, time: 212.101s, loss: 0.717, train accuracy: 0.703\n","epoch: 3, time: 215.342s, loss: 0.716, train accuracy: 0.742\n","epoch: 3, time: 218.580s, loss: 0.795, train accuracy: 0.703\n","epoch: 3, time: 221.799s, loss: 0.696, train accuracy: 0.750\n","epoch: 3, time: 225.030s, loss: 0.776, train accuracy: 0.727\n","epoch: 3, time: 228.263s, loss: 0.553, train accuracy: 0.789\n","epoch: 3, time: 231.494s, loss: 0.662, train accuracy: 0.805\n","epoch: 3, time: 234.723s, loss: 0.769, train accuracy: 0.734\n","epoch: 3, time: 237.934s, loss: 0.629, train accuracy: 0.812\n","epoch: 3, time: 241.160s, loss: 0.641, train accuracy: 0.781\n","epoch: 3, time: 244.407s, loss: 0.595, train accuracy: 0.812\n","epoch: 3, time: 247.630s, loss: 0.741, train accuracy: 0.750\n","epoch: 3, time: 250.854s, loss: 0.634, train accuracy: 0.750\n","epoch: 3, time: 254.079s, loss: 0.585, train accuracy: 0.789\n","epoch: 3, time: 257.333s, loss: 0.771, train accuracy: 0.703\n","epoch: 3, time: 260.551s, loss: 0.725, train accuracy: 0.750\n","epoch: 3, time: 263.792s, loss: 0.749, train accuracy: 0.734\n","epoch: 3, time: 267.033s, loss: 0.660, train accuracy: 0.781\n","Accuracy on the test set: 0.762\n","epoch: 4, time: 274.103s, loss: 0.570, train accuracy: 0.836\n","epoch: 4, time: 277.313s, loss: 0.516, train accuracy: 0.773\n","epoch: 4, time: 280.540s, loss: 0.551, train accuracy: 0.820\n","epoch: 4, time: 283.780s, loss: 0.669, train accuracy: 0.781\n","epoch: 4, time: 287.037s, loss: 0.668, train accuracy: 0.773\n","epoch: 4, time: 290.279s, loss: 0.671, train accuracy: 0.766\n","epoch: 4, time: 293.533s, loss: 0.589, train accuracy: 0.812\n","epoch: 4, time: 296.781s, loss: 0.463, train accuracy: 0.859\n","epoch: 4, time: 300.013s, loss: 0.632, train accuracy: 0.758\n","epoch: 4, time: 303.230s, loss: 0.565, train accuracy: 0.781\n","epoch: 4, time: 306.463s, loss: 0.544, train accuracy: 0.797\n","epoch: 4, time: 309.678s, loss: 0.619, train accuracy: 0.773\n","epoch: 4, time: 312.905s, loss: 0.636, train accuracy: 0.766\n","epoch: 4, time: 316.145s, loss: 0.500, train accuracy: 0.766\n","epoch: 4, time: 319.381s, loss: 0.776, train accuracy: 0.734\n","epoch: 4, time: 322.601s, loss: 0.586, train accuracy: 0.797\n","epoch: 4, time: 325.841s, loss: 0.559, train accuracy: 0.789\n","epoch: 4, time: 329.077s, loss: 0.740, train accuracy: 0.727\n","epoch: 4, time: 332.307s, loss: 0.523, train accuracy: 0.797\n","epoch: 4, time: 335.544s, loss: 0.467, train accuracy: 0.836\n","Accuracy on the test set: 0.786\n","epoch: 5, time: 342.684s, loss: 0.535, train accuracy: 0.820\n","epoch: 5, time: 345.925s, loss: 0.592, train accuracy: 0.789\n","epoch: 5, time: 349.169s, loss: 0.560, train accuracy: 0.773\n","epoch: 5, time: 352.417s, loss: 0.470, train accuracy: 0.859\n","epoch: 5, time: 355.667s, loss: 0.599, train accuracy: 0.789\n","epoch: 5, time: 358.893s, loss: 0.720, train accuracy: 0.766\n","epoch: 5, time: 362.133s, loss: 0.546, train accuracy: 0.836\n","epoch: 5, time: 365.363s, loss: 0.636, train accuracy: 0.773\n","epoch: 5, time: 368.590s, loss: 0.733, train accuracy: 0.719\n","epoch: 5, time: 371.828s, loss: 0.769, train accuracy: 0.727\n","epoch: 5, time: 375.071s, loss: 0.422, train accuracy: 0.828\n","epoch: 5, time: 378.340s, loss: 0.746, train accuracy: 0.703\n","epoch: 5, time: 381.592s, loss: 0.604, train accuracy: 0.750\n","epoch: 5, time: 384.814s, loss: 0.555, train accuracy: 0.836\n","epoch: 5, time: 388.034s, loss: 0.703, train accuracy: 0.781\n","epoch: 5, time: 391.280s, loss: 0.459, train accuracy: 0.844\n","epoch: 5, time: 394.502s, loss: 0.569, train accuracy: 0.781\n","epoch: 5, time: 397.739s, loss: 0.421, train accuracy: 0.844\n","epoch: 5, time: 400.985s, loss: 0.432, train accuracy: 0.891\n","epoch: 5, time: 404.203s, loss: 0.683, train accuracy: 0.773\n","Accuracy on the test set: 0.804\n","epoch: 6, time: 411.339s, loss: 0.560, train accuracy: 0.805\n","epoch: 6, time: 414.570s, loss: 0.493, train accuracy: 0.836\n","epoch: 6, time: 417.796s, loss: 0.544, train accuracy: 0.844\n","epoch: 6, time: 421.009s, loss: 0.571, train accuracy: 0.773\n","epoch: 6, time: 424.258s, loss: 0.462, train accuracy: 0.844\n","epoch: 6, time: 427.499s, loss: 0.488, train accuracy: 0.812\n","epoch: 6, time: 430.729s, loss: 0.608, train accuracy: 0.750\n","epoch: 6, time: 433.978s, loss: 0.461, train accuracy: 0.828\n","epoch: 6, time: 437.229s, loss: 0.514, train accuracy: 0.836\n","epoch: 6, time: 440.451s, loss: 0.375, train accuracy: 0.867\n","epoch: 6, time: 443.724s, loss: 0.432, train accuracy: 0.883\n","epoch: 6, time: 446.942s, loss: 0.399, train accuracy: 0.867\n","epoch: 6, time: 450.148s, loss: 0.489, train accuracy: 0.812\n","epoch: 6, time: 453.404s, loss: 0.401, train accuracy: 0.867\n","epoch: 6, time: 456.626s, loss: 0.572, train accuracy: 0.828\n","epoch: 6, time: 459.846s, loss: 0.505, train accuracy: 0.828\n","epoch: 6, time: 463.090s, loss: 0.524, train accuracy: 0.812\n","epoch: 6, time: 466.331s, loss: 0.582, train accuracy: 0.820\n","epoch: 6, time: 469.564s, loss: 0.540, train accuracy: 0.750\n","epoch: 6, time: 472.777s, loss: 0.479, train accuracy: 0.797\n","Accuracy on the test set: 0.783\n","epoch: 7, time: 479.841s, loss: 0.412, train accuracy: 0.844\n","epoch: 7, time: 483.069s, loss: 0.504, train accuracy: 0.797\n","epoch: 7, time: 486.315s, loss: 0.595, train accuracy: 0.773\n","epoch: 7, time: 489.535s, loss: 0.521, train accuracy: 0.844\n","epoch: 7, time: 492.730s, loss: 0.456, train accuracy: 0.844\n","epoch: 7, time: 495.980s, loss: 0.445, train accuracy: 0.836\n","epoch: 7, time: 499.185s, loss: 0.486, train accuracy: 0.836\n","epoch: 7, time: 502.403s, loss: 0.500, train accuracy: 0.805\n","epoch: 7, time: 505.651s, loss: 0.521, train accuracy: 0.820\n","epoch: 7, time: 508.874s, loss: 0.438, train accuracy: 0.844\n","epoch: 7, time: 512.072s, loss: 0.648, train accuracy: 0.805\n","epoch: 7, time: 515.305s, loss: 0.531, train accuracy: 0.836\n","epoch: 7, time: 518.520s, loss: 0.496, train accuracy: 0.805\n","epoch: 7, time: 521.731s, loss: 0.610, train accuracy: 0.812\n","epoch: 7, time: 524.999s, loss: 0.395, train accuracy: 0.859\n","epoch: 7, time: 528.248s, loss: 0.438, train accuracy: 0.836\n","epoch: 7, time: 531.479s, loss: 0.436, train accuracy: 0.867\n","epoch: 7, time: 534.697s, loss: 0.557, train accuracy: 0.797\n","epoch: 7, time: 537.905s, loss: 0.525, train accuracy: 0.828\n","epoch: 7, time: 541.114s, loss: 0.541, train accuracy: 0.812\n","Accuracy on the test set: 0.792\n","epoch: 8, time: 548.195s, loss: 0.554, train accuracy: 0.805\n","epoch: 8, time: 551.418s, loss: 0.399, train accuracy: 0.875\n","epoch: 8, time: 554.649s, loss: 0.482, train accuracy: 0.859\n","epoch: 8, time: 557.875s, loss: 0.416, train accuracy: 0.891\n","epoch: 8, time: 561.095s, loss: 0.518, train accuracy: 0.836\n","epoch: 8, time: 564.310s, loss: 0.510, train accuracy: 0.805\n","epoch: 8, time: 567.551s, loss: 0.384, train accuracy: 0.867\n","epoch: 8, time: 570.752s, loss: 0.439, train accuracy: 0.844\n","epoch: 8, time: 573.949s, loss: 0.320, train accuracy: 0.859\n","epoch: 8, time: 577.162s, loss: 0.420, train accuracy: 0.875\n","epoch: 8, time: 580.371s, loss: 0.419, train accuracy: 0.844\n","epoch: 8, time: 583.582s, loss: 0.424, train accuracy: 0.828\n","epoch: 8, time: 586.802s, loss: 0.487, train accuracy: 0.828\n","epoch: 8, time: 590.014s, loss: 0.489, train accuracy: 0.828\n","epoch: 8, time: 593.247s, loss: 0.435, train accuracy: 0.875\n","epoch: 8, time: 596.474s, loss: 0.564, train accuracy: 0.820\n","epoch: 8, time: 599.688s, loss: 0.601, train accuracy: 0.805\n","epoch: 8, time: 602.901s, loss: 0.685, train accuracy: 0.773\n","epoch: 8, time: 606.117s, loss: 0.353, train accuracy: 0.883\n","epoch: 8, time: 609.326s, loss: 0.401, train accuracy: 0.805\n","Accuracy on the test set: 0.832\n","epoch: 9, time: 616.410s, loss: 0.638, train accuracy: 0.719\n","epoch: 9, time: 619.660s, loss: 0.575, train accuracy: 0.828\n","epoch: 9, time: 622.882s, loss: 0.483, train accuracy: 0.836\n","epoch: 9, time: 626.113s, loss: 0.558, train accuracy: 0.828\n","epoch: 9, time: 629.350s, loss: 0.425, train accuracy: 0.891\n","epoch: 9, time: 632.575s, loss: 0.423, train accuracy: 0.859\n","epoch: 9, time: 635.773s, loss: 0.372, train accuracy: 0.891\n","epoch: 9, time: 639.001s, loss: 0.392, train accuracy: 0.883\n","epoch: 9, time: 642.211s, loss: 0.532, train accuracy: 0.812\n","epoch: 9, time: 645.436s, loss: 0.539, train accuracy: 0.789\n","epoch: 9, time: 648.684s, loss: 0.490, train accuracy: 0.820\n","epoch: 9, time: 651.916s, loss: 0.278, train accuracy: 0.906\n","epoch: 9, time: 655.146s, loss: 0.376, train accuracy: 0.867\n","epoch: 9, time: 658.364s, loss: 0.362, train accuracy: 0.883\n","epoch: 9, time: 661.611s, loss: 0.346, train accuracy: 0.883\n","epoch: 9, time: 664.839s, loss: 0.401, train accuracy: 0.875\n","epoch: 9, time: 668.057s, loss: 0.435, train accuracy: 0.844\n","epoch: 9, time: 671.265s, loss: 0.329, train accuracy: 0.898\n","epoch: 9, time: 674.495s, loss: 0.299, train accuracy: 0.914\n","epoch: 9, time: 677.725s, loss: 0.381, train accuracy: 0.852\n","Accuracy on the test set: 0.824\n","epoch: 10, time: 684.860s, loss: 0.414, train accuracy: 0.836\n","epoch: 10, time: 688.080s, loss: 0.367, train accuracy: 0.898\n","epoch: 10, time: 691.300s, loss: 0.430, train accuracy: 0.836\n","epoch: 10, time: 694.510s, loss: 0.439, train accuracy: 0.828\n","epoch: 10, time: 697.736s, loss: 0.434, train accuracy: 0.852\n","epoch: 10, time: 700.981s, loss: 0.503, train accuracy: 0.805\n","epoch: 10, time: 704.204s, loss: 0.431, train accuracy: 0.867\n","epoch: 10, time: 707.428s, loss: 0.299, train accuracy: 0.867\n","epoch: 10, time: 710.642s, loss: 0.442, train accuracy: 0.836\n","epoch: 10, time: 713.865s, loss: 0.314, train accuracy: 0.914\n","epoch: 10, time: 717.092s, loss: 0.281, train accuracy: 0.914\n","epoch: 10, time: 720.317s, loss: 0.569, train accuracy: 0.797\n","epoch: 10, time: 723.543s, loss: 0.478, train accuracy: 0.828\n","epoch: 10, time: 726.755s, loss: 0.286, train accuracy: 0.930\n","epoch: 10, time: 729.994s, loss: 0.445, train accuracy: 0.836\n","epoch: 10, time: 733.249s, loss: 0.252, train accuracy: 0.922\n","epoch: 10, time: 736.485s, loss: 0.519, train accuracy: 0.836\n","epoch: 10, time: 739.718s, loss: 0.414, train accuracy: 0.828\n","epoch: 10, time: 742.940s, loss: 0.399, train accuracy: 0.844\n","epoch: 10, time: 746.193s, loss: 0.477, train accuracy: 0.820\n","Accuracy on the test set: 0.821\n","epoch: 11, time: 753.293s, loss: 0.336, train accuracy: 0.922\n","epoch: 11, time: 756.513s, loss: 0.344, train accuracy: 0.898\n","epoch: 11, time: 759.741s, loss: 0.276, train accuracy: 0.898\n","epoch: 11, time: 762.951s, loss: 0.458, train accuracy: 0.875\n","epoch: 11, time: 766.192s, loss: 0.371, train accuracy: 0.875\n","epoch: 11, time: 769.409s, loss: 0.629, train accuracy: 0.805\n","epoch: 11, time: 772.632s, loss: 0.220, train accuracy: 0.930\n","epoch: 11, time: 775.850s, loss: 0.305, train accuracy: 0.891\n","epoch: 11, time: 779.057s, loss: 0.255, train accuracy: 0.906\n","epoch: 11, time: 782.276s, loss: 0.454, train accuracy: 0.820\n","epoch: 11, time: 785.485s, loss: 0.482, train accuracy: 0.852\n","epoch: 11, time: 788.705s, loss: 0.278, train accuracy: 0.891\n","epoch: 11, time: 791.911s, loss: 0.469, train accuracy: 0.820\n","epoch: 11, time: 795.157s, loss: 0.555, train accuracy: 0.805\n","epoch: 11, time: 798.396s, loss: 0.333, train accuracy: 0.898\n","epoch: 11, time: 801.625s, loss: 0.431, train accuracy: 0.836\n","epoch: 11, time: 804.863s, loss: 0.379, train accuracy: 0.836\n","epoch: 11, time: 808.094s, loss: 0.350, train accuracy: 0.859\n","epoch: 11, time: 811.310s, loss: 0.358, train accuracy: 0.883\n","epoch: 11, time: 814.531s, loss: 0.382, train accuracy: 0.867\n","Accuracy on the test set: 0.816\n","epoch: 12, time: 821.650s, loss: 0.477, train accuracy: 0.852\n","epoch: 12, time: 824.891s, loss: 0.275, train accuracy: 0.883\n","epoch: 12, time: 828.133s, loss: 0.330, train accuracy: 0.883\n","epoch: 12, time: 831.365s, loss: 0.355, train accuracy: 0.891\n","epoch: 12, time: 834.596s, loss: 0.343, train accuracy: 0.891\n","epoch: 12, time: 837.825s, loss: 0.323, train accuracy: 0.883\n","epoch: 12, time: 841.044s, loss: 0.259, train accuracy: 0.898\n","epoch: 12, time: 844.250s, loss: 0.354, train accuracy: 0.859\n","epoch: 12, time: 847.471s, loss: 0.371, train accuracy: 0.883\n","epoch: 12, time: 850.683s, loss: 0.321, train accuracy: 0.922\n","epoch: 12, time: 853.919s, loss: 0.295, train accuracy: 0.898\n","epoch: 12, time: 857.150s, loss: 0.283, train accuracy: 0.906\n","epoch: 12, time: 860.374s, loss: 0.449, train accuracy: 0.867\n","epoch: 12, time: 863.600s, loss: 0.382, train accuracy: 0.883\n","epoch: 12, time: 866.840s, loss: 0.395, train accuracy: 0.867\n","epoch: 12, time: 870.064s, loss: 0.394, train accuracy: 0.883\n","epoch: 12, time: 873.262s, loss: 0.280, train accuracy: 0.891\n","epoch: 12, time: 876.478s, loss: 0.427, train accuracy: 0.867\n","epoch: 12, time: 879.685s, loss: 0.307, train accuracy: 0.891\n","epoch: 12, time: 882.900s, loss: 0.424, train accuracy: 0.852\n","Accuracy on the test set: 0.848\n","epoch: 13, time: 890.007s, loss: 0.311, train accuracy: 0.906\n","epoch: 13, time: 893.239s, loss: 0.424, train accuracy: 0.828\n","epoch: 13, time: 896.454s, loss: 0.501, train accuracy: 0.805\n","epoch: 13, time: 899.695s, loss: 0.333, train accuracy: 0.859\n","epoch: 13, time: 902.904s, loss: 0.309, train accuracy: 0.867\n","epoch: 13, time: 906.115s, loss: 0.269, train accuracy: 0.891\n","epoch: 13, time: 909.345s, loss: 0.291, train accuracy: 0.883\n","epoch: 13, time: 912.563s, loss: 0.361, train accuracy: 0.875\n","epoch: 13, time: 915.799s, loss: 0.332, train accuracy: 0.852\n","epoch: 13, time: 919.045s, loss: 0.390, train accuracy: 0.844\n","epoch: 13, time: 922.268s, loss: 0.285, train accuracy: 0.922\n","epoch: 13, time: 925.505s, loss: 0.310, train accuracy: 0.938\n","epoch: 13, time: 928.738s, loss: 0.275, train accuracy: 0.922\n","epoch: 13, time: 931.952s, loss: 0.319, train accuracy: 0.891\n","epoch: 13, time: 935.188s, loss: 0.352, train accuracy: 0.859\n","epoch: 13, time: 938.387s, loss: 0.315, train accuracy: 0.906\n","epoch: 13, time: 941.604s, loss: 0.288, train accuracy: 0.898\n","epoch: 13, time: 944.858s, loss: 0.421, train accuracy: 0.875\n","epoch: 13, time: 948.076s, loss: 0.364, train accuracy: 0.883\n","epoch: 13, time: 951.311s, loss: 0.369, train accuracy: 0.875\n","Accuracy on the test set: 0.846\n","epoch: 14, time: 958.392s, loss: 0.264, train accuracy: 0.945\n","epoch: 14, time: 961.598s, loss: 0.301, train accuracy: 0.883\n","epoch: 14, time: 964.812s, loss: 0.254, train accuracy: 0.898\n","epoch: 14, time: 968.039s, loss: 0.354, train accuracy: 0.852\n","epoch: 14, time: 971.260s, loss: 0.260, train accuracy: 0.914\n","epoch: 14, time: 974.488s, loss: 0.314, train accuracy: 0.898\n","epoch: 14, time: 977.742s, loss: 0.231, train accuracy: 0.945\n","epoch: 14, time: 980.985s, loss: 0.368, train accuracy: 0.859\n","epoch: 14, time: 984.210s, loss: 0.190, train accuracy: 0.930\n","epoch: 14, time: 987.440s, loss: 0.289, train accuracy: 0.898\n","epoch: 14, time: 990.666s, loss: 0.291, train accuracy: 0.852\n","epoch: 14, time: 993.861s, loss: 0.205, train accuracy: 0.914\n","epoch: 14, time: 997.071s, loss: 0.287, train accuracy: 0.891\n","epoch: 14, time: 1000.284s, loss: 0.210, train accuracy: 0.914\n","epoch: 14, time: 1003.470s, loss: 0.364, train accuracy: 0.867\n","epoch: 14, time: 1006.698s, loss: 0.204, train accuracy: 0.922\n","epoch: 14, time: 1009.906s, loss: 0.342, train accuracy: 0.875\n","epoch: 14, time: 1013.137s, loss: 0.385, train accuracy: 0.891\n","epoch: 14, time: 1016.349s, loss: 0.240, train accuracy: 0.906\n","epoch: 14, time: 1019.561s, loss: 0.317, train accuracy: 0.898\n","Accuracy on the test set: 0.855\n","epoch: 15, time: 1026.640s, loss: 0.274, train accuracy: 0.906\n","epoch: 15, time: 1029.855s, loss: 0.408, train accuracy: 0.898\n","epoch: 15, time: 1033.064s, loss: 0.314, train accuracy: 0.891\n","epoch: 15, time: 1036.293s, loss: 0.300, train accuracy: 0.852\n","epoch: 15, time: 1039.523s, loss: 0.304, train accuracy: 0.914\n","epoch: 15, time: 1042.755s, loss: 0.181, train accuracy: 0.945\n","epoch: 15, time: 1045.981s, loss: 0.234, train accuracy: 0.922\n","epoch: 15, time: 1049.221s, loss: 0.358, train accuracy: 0.891\n","epoch: 15, time: 1052.473s, loss: 0.427, train accuracy: 0.852\n","epoch: 15, time: 1055.740s, loss: 0.448, train accuracy: 0.812\n","epoch: 15, time: 1058.984s, loss: 0.353, train accuracy: 0.883\n","epoch: 15, time: 1062.204s, loss: 0.325, train accuracy: 0.891\n","epoch: 15, time: 1065.450s, loss: 0.224, train accuracy: 0.898\n","epoch: 15, time: 1068.727s, loss: 0.271, train accuracy: 0.844\n","epoch: 15, time: 1071.974s, loss: 0.205, train accuracy: 0.938\n","epoch: 15, time: 1075.237s, loss: 0.309, train accuracy: 0.891\n","epoch: 15, time: 1078.492s, loss: 0.305, train accuracy: 0.891\n","epoch: 15, time: 1081.757s, loss: 0.481, train accuracy: 0.797\n","epoch: 15, time: 1085.023s, loss: 0.407, train accuracy: 0.844\n","epoch: 15, time: 1088.315s, loss: 0.225, train accuracy: 0.914\n","Accuracy on the test set: 0.857\n","epoch: 16, time: 1095.520s, loss: 0.291, train accuracy: 0.906\n","epoch: 16, time: 1098.759s, loss: 0.248, train accuracy: 0.938\n","epoch: 16, time: 1101.985s, loss: 0.340, train accuracy: 0.867\n","epoch: 16, time: 1105.224s, loss: 0.366, train accuracy: 0.852\n","epoch: 16, time: 1108.473s, loss: 0.314, train accuracy: 0.867\n","epoch: 16, time: 1111.727s, loss: 0.298, train accuracy: 0.891\n","epoch: 16, time: 1114.971s, loss: 0.231, train accuracy: 0.953\n","epoch: 16, time: 1118.198s, loss: 0.223, train accuracy: 0.914\n","epoch: 16, time: 1121.419s, loss: 0.204, train accuracy: 0.938\n","epoch: 16, time: 1124.647s, loss: 0.257, train accuracy: 0.930\n","epoch: 16, time: 1127.896s, loss: 0.343, train accuracy: 0.859\n","epoch: 16, time: 1131.137s, loss: 0.341, train accuracy: 0.883\n","epoch: 16, time: 1134.370s, loss: 0.225, train accuracy: 0.906\n","epoch: 16, time: 1137.611s, loss: 0.341, train accuracy: 0.859\n","epoch: 16, time: 1140.867s, loss: 0.251, train accuracy: 0.914\n","epoch: 16, time: 1144.102s, loss: 0.343, train accuracy: 0.883\n","epoch: 16, time: 1147.373s, loss: 0.331, train accuracy: 0.852\n","epoch: 16, time: 1150.648s, loss: 0.304, train accuracy: 0.891\n","epoch: 16, time: 1153.913s, loss: 0.240, train accuracy: 0.914\n","epoch: 16, time: 1157.147s, loss: 0.241, train accuracy: 0.938\n","Accuracy on the test set: 0.868\n","epoch: 17, time: 1164.322s, loss: 0.140, train accuracy: 0.914\n","epoch: 17, time: 1167.565s, loss: 0.271, train accuracy: 0.906\n","epoch: 17, time: 1170.801s, loss: 0.220, train accuracy: 0.938\n","epoch: 17, time: 1174.022s, loss: 0.213, train accuracy: 0.914\n","epoch: 17, time: 1177.245s, loss: 0.318, train accuracy: 0.875\n","epoch: 17, time: 1180.469s, loss: 0.213, train accuracy: 0.938\n","epoch: 17, time: 1183.717s, loss: 0.234, train accuracy: 0.922\n","epoch: 17, time: 1186.956s, loss: 0.371, train accuracy: 0.852\n","epoch: 17, time: 1190.197s, loss: 0.323, train accuracy: 0.914\n","epoch: 17, time: 1193.438s, loss: 0.343, train accuracy: 0.891\n","epoch: 17, time: 1196.697s, loss: 0.349, train accuracy: 0.859\n","epoch: 17, time: 1199.936s, loss: 0.312, train accuracy: 0.891\n","epoch: 17, time: 1203.169s, loss: 0.241, train accuracy: 0.930\n","epoch: 17, time: 1206.389s, loss: 0.266, train accuracy: 0.914\n","epoch: 17, time: 1209.627s, loss: 0.326, train accuracy: 0.875\n","epoch: 17, time: 1212.844s, loss: 0.233, train accuracy: 0.930\n","epoch: 17, time: 1216.089s, loss: 0.291, train accuracy: 0.922\n","epoch: 17, time: 1219.333s, loss: 0.335, train accuracy: 0.867\n","epoch: 17, time: 1222.577s, loss: 0.261, train accuracy: 0.906\n","epoch: 17, time: 1225.829s, loss: 0.216, train accuracy: 0.922\n","Accuracy on the test set: 0.865\n","epoch: 18, time: 1232.939s, loss: 0.135, train accuracy: 0.945\n","epoch: 18, time: 1236.154s, loss: 0.245, train accuracy: 0.914\n","epoch: 18, time: 1239.387s, loss: 0.200, train accuracy: 0.938\n","epoch: 18, time: 1242.606s, loss: 0.321, train accuracy: 0.883\n","epoch: 18, time: 1245.844s, loss: 0.321, train accuracy: 0.891\n","epoch: 18, time: 1249.068s, loss: 0.184, train accuracy: 0.961\n","epoch: 18, time: 1252.304s, loss: 0.345, train accuracy: 0.875\n","epoch: 18, time: 1255.527s, loss: 0.266, train accuracy: 0.898\n","epoch: 18, time: 1258.741s, loss: 0.235, train accuracy: 0.914\n","epoch: 18, time: 1261.953s, loss: 0.232, train accuracy: 0.922\n","epoch: 18, time: 1265.193s, loss: 0.277, train accuracy: 0.922\n","epoch: 18, time: 1268.421s, loss: 0.234, train accuracy: 0.891\n","epoch: 18, time: 1271.659s, loss: 0.203, train accuracy: 0.922\n","epoch: 18, time: 1274.879s, loss: 0.230, train accuracy: 0.930\n","epoch: 18, time: 1278.098s, loss: 0.251, train accuracy: 0.914\n","epoch: 18, time: 1281.358s, loss: 0.256, train accuracy: 0.891\n","epoch: 18, time: 1284.596s, loss: 0.260, train accuracy: 0.898\n","epoch: 18, time: 1287.837s, loss: 0.124, train accuracy: 0.938\n","epoch: 18, time: 1291.056s, loss: 0.291, train accuracy: 0.883\n","epoch: 18, time: 1294.282s, loss: 0.347, train accuracy: 0.891\n","Accuracy on the test set: 0.859\n","epoch: 19, time: 1301.384s, loss: 0.225, train accuracy: 0.930\n","epoch: 19, time: 1304.626s, loss: 0.239, train accuracy: 0.945\n","epoch: 19, time: 1307.871s, loss: 0.237, train accuracy: 0.891\n","epoch: 19, time: 1311.096s, loss: 0.204, train accuracy: 0.914\n","epoch: 19, time: 1314.362s, loss: 0.233, train accuracy: 0.930\n","epoch: 19, time: 1317.598s, loss: 0.178, train accuracy: 0.945\n","epoch: 19, time: 1320.824s, loss: 0.203, train accuracy: 0.930\n","epoch: 19, time: 1324.043s, loss: 0.244, train accuracy: 0.930\n","epoch: 19, time: 1327.256s, loss: 0.226, train accuracy: 0.922\n","epoch: 19, time: 1330.469s, loss: 0.297, train accuracy: 0.875\n","epoch: 19, time: 1333.705s, loss: 0.214, train accuracy: 0.930\n","epoch: 19, time: 1336.934s, loss: 0.235, train accuracy: 0.922\n","epoch: 19, time: 1340.188s, loss: 0.256, train accuracy: 0.914\n","epoch: 19, time: 1343.448s, loss: 0.158, train accuracy: 0.961\n","epoch: 19, time: 1346.683s, loss: 0.312, train accuracy: 0.883\n","epoch: 19, time: 1349.920s, loss: 0.235, train accuracy: 0.906\n","epoch: 19, time: 1353.138s, loss: 0.357, train accuracy: 0.883\n","epoch: 19, time: 1356.365s, loss: 0.202, train accuracy: 0.938\n","epoch: 19, time: 1359.606s, loss: 0.171, train accuracy: 0.938\n","epoch: 19, time: 1362.805s, loss: 0.232, train accuracy: 0.914\n","Accuracy on the test set: 0.871\n","epoch: 20, time: 1369.915s, loss: 0.152, train accuracy: 0.953\n","epoch: 20, time: 1373.150s, loss: 0.247, train accuracy: 0.930\n","epoch: 20, time: 1376.394s, loss: 0.290, train accuracy: 0.914\n","epoch: 20, time: 1379.613s, loss: 0.267, train accuracy: 0.906\n","epoch: 20, time: 1382.839s, loss: 0.159, train accuracy: 0.945\n","epoch: 20, time: 1386.058s, loss: 0.251, train accuracy: 0.922\n","epoch: 20, time: 1389.287s, loss: 0.279, train accuracy: 0.906\n","epoch: 20, time: 1392.517s, loss: 0.193, train accuracy: 0.922\n","epoch: 20, time: 1395.754s, loss: 0.252, train accuracy: 0.875\n","epoch: 20, time: 1399.000s, loss: 0.277, train accuracy: 0.891\n","epoch: 20, time: 1402.228s, loss: 0.201, train accuracy: 0.906\n","epoch: 20, time: 1405.441s, loss: 0.251, train accuracy: 0.922\n","epoch: 20, time: 1408.674s, loss: 0.196, train accuracy: 0.922\n","epoch: 20, time: 1411.894s, loss: 0.122, train accuracy: 0.984\n","epoch: 20, time: 1415.126s, loss: 0.224, train accuracy: 0.922\n","epoch: 20, time: 1418.352s, loss: 0.110, train accuracy: 0.961\n","epoch: 20, time: 1421.588s, loss: 0.187, train accuracy: 0.922\n","epoch: 20, time: 1424.817s, loss: 0.256, train accuracy: 0.898\n","epoch: 20, time: 1428.061s, loss: 0.319, train accuracy: 0.898\n","epoch: 20, time: 1431.297s, loss: 0.143, train accuracy: 0.938\n","Accuracy on the test set: 0.871\n","epoch: 21, time: 1438.411s, loss: 0.207, train accuracy: 0.945\n","epoch: 21, time: 1441.635s, loss: 0.137, train accuracy: 0.938\n","epoch: 21, time: 1444.861s, loss: 0.202, train accuracy: 0.930\n","epoch: 21, time: 1448.080s, loss: 0.125, train accuracy: 0.953\n","epoch: 21, time: 1451.303s, loss: 0.303, train accuracy: 0.898\n","epoch: 21, time: 1454.531s, loss: 0.214, train accuracy: 0.914\n","epoch: 21, time: 1457.766s, loss: 0.227, train accuracy: 0.914\n","epoch: 21, time: 1461.014s, loss: 0.268, train accuracy: 0.930\n","epoch: 21, time: 1464.253s, loss: 0.240, train accuracy: 0.922\n","epoch: 21, time: 1467.505s, loss: 0.127, train accuracy: 0.961\n","epoch: 21, time: 1470.740s, loss: 0.214, train accuracy: 0.938\n","epoch: 21, time: 1473.969s, loss: 0.252, train accuracy: 0.961\n","epoch: 21, time: 1477.199s, loss: 0.227, train accuracy: 0.930\n","epoch: 21, time: 1480.429s, loss: 0.256, train accuracy: 0.898\n","epoch: 21, time: 1483.666s, loss: 0.284, train accuracy: 0.875\n","epoch: 21, time: 1486.897s, loss: 0.295, train accuracy: 0.898\n","epoch: 21, time: 1490.160s, loss: 0.314, train accuracy: 0.891\n","epoch: 21, time: 1493.411s, loss: 0.215, train accuracy: 0.922\n","epoch: 21, time: 1496.655s, loss: 0.207, train accuracy: 0.961\n","epoch: 21, time: 1499.884s, loss: 0.178, train accuracy: 0.938\n","Accuracy on the test set: 0.870\n","epoch: 22, time: 1506.977s, loss: 0.291, train accuracy: 0.906\n","epoch: 22, time: 1510.208s, loss: 0.166, train accuracy: 0.945\n","epoch: 22, time: 1513.436s, loss: 0.227, train accuracy: 0.930\n","epoch: 22, time: 1516.686s, loss: 0.123, train accuracy: 0.969\n","epoch: 22, time: 1519.918s, loss: 0.167, train accuracy: 0.945\n","epoch: 22, time: 1523.136s, loss: 0.225, train accuracy: 0.945\n","epoch: 22, time: 1526.364s, loss: 0.108, train accuracy: 0.961\n","epoch: 22, time: 1529.578s, loss: 0.276, train accuracy: 0.906\n","epoch: 22, time: 1532.805s, loss: 0.272, train accuracy: 0.906\n","epoch: 22, time: 1536.032s, loss: 0.110, train accuracy: 0.945\n","epoch: 22, time: 1539.261s, loss: 0.177, train accuracy: 0.938\n","epoch: 22, time: 1542.478s, loss: 0.259, train accuracy: 0.914\n","epoch: 22, time: 1545.720s, loss: 0.220, train accuracy: 0.938\n","epoch: 22, time: 1548.931s, loss: 0.218, train accuracy: 0.914\n","epoch: 22, time: 1552.171s, loss: 0.249, train accuracy: 0.906\n","epoch: 22, time: 1555.395s, loss: 0.163, train accuracy: 0.938\n","epoch: 22, time: 1558.651s, loss: 0.154, train accuracy: 0.953\n","epoch: 22, time: 1561.885s, loss: 0.243, train accuracy: 0.898\n","epoch: 22, time: 1565.113s, loss: 0.349, train accuracy: 0.875\n","epoch: 22, time: 1568.331s, loss: 0.277, train accuracy: 0.914\n","Accuracy on the test set: 0.868\n","epoch: 23, time: 1575.436s, loss: 0.222, train accuracy: 0.922\n","epoch: 23, time: 1578.666s, loss: 0.113, train accuracy: 0.953\n","epoch: 23, time: 1581.906s, loss: 0.232, train accuracy: 0.898\n","epoch: 23, time: 1585.133s, loss: 0.230, train accuracy: 0.906\n","epoch: 23, time: 1588.376s, loss: 0.226, train accuracy: 0.930\n","epoch: 23, time: 1591.623s, loss: 0.245, train accuracy: 0.891\n","epoch: 23, time: 1594.842s, loss: 0.144, train accuracy: 0.953\n","epoch: 23, time: 1598.079s, loss: 0.282, train accuracy: 0.883\n","epoch: 23, time: 1601.297s, loss: 0.164, train accuracy: 0.930\n","epoch: 23, time: 1604.520s, loss: 0.236, train accuracy: 0.938\n","epoch: 23, time: 1607.764s, loss: 0.353, train accuracy: 0.867\n","epoch: 23, time: 1610.989s, loss: 0.139, train accuracy: 0.945\n","epoch: 23, time: 1614.218s, loss: 0.103, train accuracy: 0.961\n","epoch: 23, time: 1617.449s, loss: 0.186, train accuracy: 0.938\n","epoch: 23, time: 1620.692s, loss: 0.243, train accuracy: 0.914\n","epoch: 23, time: 1623.904s, loss: 0.269, train accuracy: 0.906\n","epoch: 23, time: 1627.126s, loss: 0.174, train accuracy: 0.961\n","epoch: 23, time: 1630.356s, loss: 0.274, train accuracy: 0.922\n","epoch: 23, time: 1633.589s, loss: 0.188, train accuracy: 0.922\n","epoch: 23, time: 1636.807s, loss: 0.336, train accuracy: 0.875\n","Accuracy on the test set: 0.866\n","epoch: 24, time: 1643.907s, loss: 0.212, train accuracy: 0.922\n","epoch: 24, time: 1647.168s, loss: 0.231, train accuracy: 0.906\n","epoch: 24, time: 1650.396s, loss: 0.179, train accuracy: 0.938\n","epoch: 24, time: 1653.618s, loss: 0.114, train accuracy: 0.945\n","epoch: 24, time: 1656.848s, loss: 0.201, train accuracy: 0.914\n","epoch: 24, time: 1660.070s, loss: 0.096, train accuracy: 0.969\n","epoch: 24, time: 1663.287s, loss: 0.129, train accuracy: 0.961\n","epoch: 24, time: 1666.547s, loss: 0.125, train accuracy: 0.961\n","epoch: 24, time: 1669.793s, loss: 0.203, train accuracy: 0.938\n","epoch: 24, time: 1673.026s, loss: 0.182, train accuracy: 0.914\n","epoch: 24, time: 1676.269s, loss: 0.143, train accuracy: 0.945\n","epoch: 24, time: 1679.492s, loss: 0.139, train accuracy: 0.938\n","epoch: 24, time: 1682.719s, loss: 0.299, train accuracy: 0.883\n","epoch: 24, time: 1685.936s, loss: 0.182, train accuracy: 0.930\n","epoch: 24, time: 1689.159s, loss: 0.181, train accuracy: 0.914\n","epoch: 24, time: 1692.379s, loss: 0.264, train accuracy: 0.891\n","epoch: 24, time: 1695.614s, loss: 0.204, train accuracy: 0.922\n","epoch: 24, time: 1698.857s, loss: 0.138, train accuracy: 0.953\n","epoch: 24, time: 1702.076s, loss: 0.123, train accuracy: 0.953\n","epoch: 24, time: 1705.300s, loss: 0.257, train accuracy: 0.914\n","Accuracy on the test set: 0.867\n","epoch: 25, time: 1712.372s, loss: 0.204, train accuracy: 0.930\n","epoch: 25, time: 1715.598s, loss: 0.216, train accuracy: 0.914\n","epoch: 25, time: 1718.835s, loss: 0.155, train accuracy: 0.938\n","epoch: 25, time: 1722.053s, loss: 0.122, train accuracy: 0.953\n","epoch: 25, time: 1725.292s, loss: 0.124, train accuracy: 0.945\n","epoch: 25, time: 1728.515s, loss: 0.274, train accuracy: 0.914\n","epoch: 25, time: 1731.754s, loss: 0.117, train accuracy: 0.938\n","epoch: 25, time: 1734.995s, loss: 0.156, train accuracy: 0.930\n","epoch: 25, time: 1738.213s, loss: 0.143, train accuracy: 0.945\n","epoch: 25, time: 1741.441s, loss: 0.225, train accuracy: 0.930\n","epoch: 25, time: 1744.669s, loss: 0.125, train accuracy: 0.969\n","epoch: 25, time: 1747.912s, loss: 0.137, train accuracy: 0.953\n","epoch: 25, time: 1751.149s, loss: 0.209, train accuracy: 0.922\n","epoch: 25, time: 1754.383s, loss: 0.199, train accuracy: 0.969\n","epoch: 25, time: 1757.614s, loss: 0.094, train accuracy: 0.961\n","epoch: 25, time: 1760.848s, loss: 0.141, train accuracy: 0.953\n","epoch: 25, time: 1764.080s, loss: 0.179, train accuracy: 0.945\n","epoch: 25, time: 1767.316s, loss: 0.202, train accuracy: 0.922\n","epoch: 25, time: 1770.531s, loss: 0.287, train accuracy: 0.906\n","epoch: 25, time: 1773.765s, loss: 0.234, train accuracy: 0.914\n","Accuracy on the test set: 0.866\n","epoch: 26, time: 1780.885s, loss: 0.249, train accuracy: 0.914\n","epoch: 26, time: 1784.102s, loss: 0.197, train accuracy: 0.922\n","epoch: 26, time: 1787.362s, loss: 0.108, train accuracy: 0.953\n","epoch: 26, time: 1790.581s, loss: 0.174, train accuracy: 0.953\n","epoch: 26, time: 1793.842s, loss: 0.224, train accuracy: 0.922\n","epoch: 26, time: 1797.084s, loss: 0.196, train accuracy: 0.922\n","epoch: 26, time: 1800.323s, loss: 0.128, train accuracy: 0.945\n","epoch: 26, time: 1803.554s, loss: 0.139, train accuracy: 0.945\n","epoch: 26, time: 1806.773s, loss: 0.145, train accuracy: 0.945\n","epoch: 26, time: 1810.000s, loss: 0.250, train accuracy: 0.938\n","epoch: 26, time: 1813.247s, loss: 0.087, train accuracy: 0.969\n","epoch: 26, time: 1816.491s, loss: 0.192, train accuracy: 0.945\n","epoch: 26, time: 1819.700s, loss: 0.189, train accuracy: 0.930\n","epoch: 26, time: 1822.926s, loss: 0.194, train accuracy: 0.914\n","epoch: 26, time: 1826.161s, loss: 0.092, train accuracy: 0.969\n","epoch: 26, time: 1829.377s, loss: 0.185, train accuracy: 0.930\n","epoch: 26, time: 1832.596s, loss: 0.168, train accuracy: 0.953\n","epoch: 26, time: 1835.816s, loss: 0.120, train accuracy: 0.961\n","epoch: 26, time: 1839.053s, loss: 0.179, train accuracy: 0.914\n","epoch: 26, time: 1842.273s, loss: 0.101, train accuracy: 0.953\n","Accuracy on the test set: 0.882\n","epoch: 27, time: 1849.373s, loss: 0.162, train accuracy: 0.953\n","epoch: 27, time: 1852.598s, loss: 0.164, train accuracy: 0.930\n","epoch: 27, time: 1855.847s, loss: 0.096, train accuracy: 0.961\n","epoch: 27, time: 1859.069s, loss: 0.214, train accuracy: 0.930\n","epoch: 27, time: 1862.294s, loss: 0.206, train accuracy: 0.930\n","epoch: 27, time: 1865.501s, loss: 0.234, train accuracy: 0.914\n","epoch: 27, time: 1868.730s, loss: 0.114, train accuracy: 0.969\n","epoch: 27, time: 1871.955s, loss: 0.133, train accuracy: 0.961\n","epoch: 27, time: 1875.192s, loss: 0.157, train accuracy: 0.922\n","epoch: 27, time: 1878.432s, loss: 0.121, train accuracy: 0.953\n","epoch: 27, time: 1881.666s, loss: 0.178, train accuracy: 0.953\n","epoch: 27, time: 1884.886s, loss: 0.190, train accuracy: 0.938\n","epoch: 27, time: 1888.112s, loss: 0.295, train accuracy: 0.906\n","epoch: 27, time: 1891.337s, loss: 0.248, train accuracy: 0.914\n","epoch: 27, time: 1894.564s, loss: 0.225, train accuracy: 0.938\n","epoch: 27, time: 1897.801s, loss: 0.136, train accuracy: 0.961\n","epoch: 27, time: 1901.030s, loss: 0.137, train accuracy: 0.930\n","epoch: 27, time: 1904.266s, loss: 0.150, train accuracy: 0.938\n","epoch: 27, time: 1907.527s, loss: 0.227, train accuracy: 0.914\n","epoch: 27, time: 1910.763s, loss: 0.116, train accuracy: 0.969\n","Accuracy on the test set: 0.873\n","epoch: 28, time: 1917.823s, loss: 0.176, train accuracy: 0.945\n","epoch: 28, time: 1921.044s, loss: 0.066, train accuracy: 0.984\n","epoch: 28, time: 1924.266s, loss: 0.165, train accuracy: 0.938\n","epoch: 28, time: 1927.479s, loss: 0.106, train accuracy: 0.945\n","epoch: 28, time: 1930.717s, loss: 0.093, train accuracy: 0.961\n","epoch: 28, time: 1933.944s, loss: 0.107, train accuracy: 0.953\n","epoch: 28, time: 1937.195s, loss: 0.155, train accuracy: 0.922\n","epoch: 28, time: 1940.410s, loss: 0.247, train accuracy: 0.891\n","epoch: 28, time: 1943.636s, loss: 0.158, train accuracy: 0.930\n","epoch: 28, time: 1946.875s, loss: 0.167, train accuracy: 0.945\n","epoch: 28, time: 1950.121s, loss: 0.118, train accuracy: 0.969\n","epoch: 28, time: 1953.375s, loss: 0.205, train accuracy: 0.922\n","epoch: 28, time: 1956.623s, loss: 0.167, train accuracy: 0.953\n","epoch: 28, time: 1959.862s, loss: 0.214, train accuracy: 0.922\n","epoch: 28, time: 1963.085s, loss: 0.178, train accuracy: 0.930\n","epoch: 28, time: 1966.322s, loss: 0.087, train accuracy: 0.977\n","epoch: 28, time: 1969.543s, loss: 0.144, train accuracy: 0.930\n","epoch: 28, time: 1972.786s, loss: 0.197, train accuracy: 0.914\n","epoch: 28, time: 1976.030s, loss: 0.097, train accuracy: 0.961\n","epoch: 28, time: 1979.282s, loss: 0.222, train accuracy: 0.938\n","Accuracy on the test set: 0.864\n","epoch: 29, time: 1986.353s, loss: 0.123, train accuracy: 0.969\n","epoch: 29, time: 1989.577s, loss: 0.168, train accuracy: 0.938\n","epoch: 29, time: 1992.794s, loss: 0.149, train accuracy: 0.930\n","epoch: 29, time: 1996.033s, loss: 0.178, train accuracy: 0.945\n","epoch: 29, time: 1999.258s, loss: 0.099, train accuracy: 0.961\n","epoch: 29, time: 2002.479s, loss: 0.170, train accuracy: 0.938\n","epoch: 29, time: 2005.718s, loss: 0.094, train accuracy: 0.977\n","epoch: 29, time: 2008.963s, loss: 0.076, train accuracy: 0.961\n","epoch: 29, time: 2012.190s, loss: 0.232, train accuracy: 0.922\n","epoch: 29, time: 2015.419s, loss: 0.155, train accuracy: 0.938\n","epoch: 29, time: 2018.658s, loss: 0.108, train accuracy: 0.969\n","epoch: 29, time: 2021.882s, loss: 0.182, train accuracy: 0.922\n","epoch: 29, time: 2025.120s, loss: 0.097, train accuracy: 0.953\n","epoch: 29, time: 2028.363s, loss: 0.129, train accuracy: 0.969\n","epoch: 29, time: 2031.622s, loss: 0.129, train accuracy: 0.953\n","epoch: 29, time: 2034.861s, loss: 0.140, train accuracy: 0.938\n","epoch: 29, time: 2038.073s, loss: 0.140, train accuracy: 0.961\n","epoch: 29, time: 2041.303s, loss: 0.271, train accuracy: 0.922\n","epoch: 29, time: 2044.523s, loss: 0.168, train accuracy: 0.953\n","epoch: 29, time: 2047.734s, loss: 0.146, train accuracy: 0.945\n","Accuracy on the test set: 0.881\n","epoch: 30, time: 2054.820s, loss: 0.107, train accuracy: 0.977\n","epoch: 30, time: 2058.060s, loss: 0.107, train accuracy: 0.961\n","epoch: 30, time: 2061.300s, loss: 0.088, train accuracy: 0.977\n","epoch: 30, time: 2064.535s, loss: 0.050, train accuracy: 0.984\n","epoch: 30, time: 2067.764s, loss: 0.094, train accuracy: 0.961\n","epoch: 30, time: 2070.979s, loss: 0.145, train accuracy: 0.930\n","epoch: 30, time: 2074.214s, loss: 0.138, train accuracy: 0.945\n","epoch: 30, time: 2077.430s, loss: 0.108, train accuracy: 0.961\n","epoch: 30, time: 2080.662s, loss: 0.161, train accuracy: 0.953\n","epoch: 30, time: 2083.887s, loss: 0.165, train accuracy: 0.953\n","epoch: 30, time: 2087.111s, loss: 0.123, train accuracy: 0.945\n","epoch: 30, time: 2090.348s, loss: 0.125, train accuracy: 0.945\n","epoch: 30, time: 2093.594s, loss: 0.170, train accuracy: 0.930\n","epoch: 30, time: 2096.819s, loss: 0.149, train accuracy: 0.930\n","epoch: 30, time: 2100.049s, loss: 0.163, train accuracy: 0.945\n","epoch: 30, time: 2103.279s, loss: 0.103, train accuracy: 0.945\n","epoch: 30, time: 2106.505s, loss: 0.236, train accuracy: 0.898\n","epoch: 30, time: 2109.744s, loss: 0.181, train accuracy: 0.938\n","epoch: 30, time: 2112.961s, loss: 0.130, train accuracy: 0.953\n","epoch: 30, time: 2116.203s, loss: 0.185, train accuracy: 0.930\n","Accuracy on the test set: 0.872\n","epoch: 31, time: 2123.283s, loss: 0.132, train accuracy: 0.945\n","epoch: 31, time: 2126.514s, loss: 0.132, train accuracy: 0.945\n","epoch: 31, time: 2129.734s, loss: 0.126, train accuracy: 0.961\n","epoch: 31, time: 2132.954s, loss: 0.107, train accuracy: 0.961\n","epoch: 31, time: 2136.194s, loss: 0.156, train accuracy: 0.953\n","epoch: 31, time: 2139.410s, loss: 0.094, train accuracy: 0.953\n","epoch: 31, time: 2142.643s, loss: 0.081, train accuracy: 0.977\n","epoch: 31, time: 2145.896s, loss: 0.211, train accuracy: 0.922\n","epoch: 31, time: 2149.110s, loss: 0.119, train accuracy: 0.953\n","epoch: 31, time: 2152.345s, loss: 0.202, train accuracy: 0.930\n","epoch: 31, time: 2155.585s, loss: 0.109, train accuracy: 0.969\n","epoch: 31, time: 2158.799s, loss: 0.099, train accuracy: 0.953\n","epoch: 31, time: 2162.014s, loss: 0.108, train accuracy: 0.961\n","epoch: 31, time: 2165.262s, loss: 0.142, train accuracy: 0.961\n","epoch: 31, time: 2168.497s, loss: 0.172, train accuracy: 0.930\n","epoch: 31, time: 2171.735s, loss: 0.220, train accuracy: 0.945\n","epoch: 31, time: 2174.962s, loss: 0.152, train accuracy: 0.945\n","epoch: 31, time: 2178.201s, loss: 0.079, train accuracy: 0.977\n","epoch: 31, time: 2181.438s, loss: 0.143, train accuracy: 0.945\n","epoch: 31, time: 2184.667s, loss: 0.168, train accuracy: 0.961\n","Accuracy on the test set: 0.856\n","epoch: 32, time: 2191.700s, loss: 0.108, train accuracy: 0.953\n","epoch: 32, time: 2194.931s, loss: 0.123, train accuracy: 0.945\n","epoch: 32, time: 2198.161s, loss: 0.085, train accuracy: 0.969\n","epoch: 32, time: 2201.391s, loss: 0.093, train accuracy: 0.977\n","epoch: 32, time: 2204.604s, loss: 0.145, train accuracy: 0.938\n","epoch: 32, time: 2207.863s, loss: 0.149, train accuracy: 0.945\n","epoch: 32, time: 2211.106s, loss: 0.082, train accuracy: 0.953\n","epoch: 32, time: 2214.344s, loss: 0.102, train accuracy: 0.984\n","epoch: 32, time: 2217.578s, loss: 0.155, train accuracy: 0.930\n","epoch: 32, time: 2220.814s, loss: 0.121, train accuracy: 0.945\n","epoch: 32, time: 2224.024s, loss: 0.124, train accuracy: 0.953\n","epoch: 32, time: 2227.248s, loss: 0.111, train accuracy: 0.953\n","epoch: 32, time: 2230.473s, loss: 0.114, train accuracy: 0.961\n","epoch: 32, time: 2233.708s, loss: 0.111, train accuracy: 0.953\n","epoch: 32, time: 2236.920s, loss: 0.110, train accuracy: 0.969\n","epoch: 32, time: 2240.149s, loss: 0.083, train accuracy: 0.969\n","epoch: 32, time: 2243.382s, loss: 0.161, train accuracy: 0.938\n","epoch: 32, time: 2246.600s, loss: 0.089, train accuracy: 0.977\n","epoch: 32, time: 2249.818s, loss: 0.094, train accuracy: 0.969\n","epoch: 32, time: 2253.044s, loss: 0.192, train accuracy: 0.914\n","Accuracy on the test set: 0.883\n","epoch: 33, time: 2260.174s, loss: 0.054, train accuracy: 0.984\n","epoch: 33, time: 2263.390s, loss: 0.117, train accuracy: 0.961\n","epoch: 33, time: 2266.630s, loss: 0.074, train accuracy: 0.969\n","epoch: 33, time: 2269.868s, loss: 0.129, train accuracy: 0.961\n","epoch: 33, time: 2273.122s, loss: 0.035, train accuracy: 0.984\n","epoch: 33, time: 2276.348s, loss: 0.096, train accuracy: 0.984\n","epoch: 33, time: 2279.574s, loss: 0.123, train accuracy: 0.953\n","epoch: 33, time: 2282.798s, loss: 0.123, train accuracy: 0.930\n","epoch: 33, time: 2286.042s, loss: 0.131, train accuracy: 0.961\n","epoch: 33, time: 2289.275s, loss: 0.110, train accuracy: 0.953\n","epoch: 33, time: 2292.520s, loss: 0.138, train accuracy: 0.945\n","epoch: 33, time: 2295.757s, loss: 0.152, train accuracy: 0.961\n","epoch: 33, time: 2298.992s, loss: 0.054, train accuracy: 0.984\n","epoch: 33, time: 2302.202s, loss: 0.099, train accuracy: 0.969\n","epoch: 33, time: 2305.421s, loss: 0.073, train accuracy: 0.977\n","epoch: 33, time: 2308.658s, loss: 0.174, train accuracy: 0.945\n","epoch: 33, time: 2311.869s, loss: 0.040, train accuracy: 0.984\n","epoch: 33, time: 2315.102s, loss: 0.101, train accuracy: 0.953\n","epoch: 33, time: 2318.332s, loss: 0.114, train accuracy: 0.969\n","epoch: 33, time: 2321.554s, loss: 0.085, train accuracy: 0.961\n","Accuracy on the test set: 0.881\n","epoch: 34, time: 2328.671s, loss: 0.051, train accuracy: 0.984\n","epoch: 34, time: 2331.900s, loss: 0.095, train accuracy: 0.953\n","epoch: 34, time: 2335.120s, loss: 0.086, train accuracy: 0.977\n","epoch: 34, time: 2338.356s, loss: 0.165, train accuracy: 0.938\n","epoch: 34, time: 2341.585s, loss: 0.118, train accuracy: 0.945\n","epoch: 34, time: 2344.814s, loss: 0.074, train accuracy: 0.977\n","epoch: 34, time: 2348.052s, loss: 0.085, train accuracy: 0.969\n","epoch: 34, time: 2351.276s, loss: 0.086, train accuracy: 0.961\n","epoch: 34, time: 2354.513s, loss: 0.174, train accuracy: 0.930\n","epoch: 34, time: 2357.755s, loss: 0.123, train accuracy: 0.953\n","epoch: 34, time: 2360.989s, loss: 0.067, train accuracy: 0.969\n","epoch: 34, time: 2364.235s, loss: 0.135, train accuracy: 0.945\n","epoch: 34, time: 2367.452s, loss: 0.145, train accuracy: 0.961\n","epoch: 34, time: 2370.675s, loss: 0.237, train accuracy: 0.914\n","epoch: 34, time: 2373.907s, loss: 0.088, train accuracy: 0.977\n","epoch: 34, time: 2377.147s, loss: 0.066, train accuracy: 0.977\n","epoch: 34, time: 2380.369s, loss: 0.220, train accuracy: 0.914\n","epoch: 34, time: 2383.596s, loss: 0.138, train accuracy: 0.953\n","epoch: 34, time: 2386.816s, loss: 0.069, train accuracy: 0.977\n","epoch: 34, time: 2390.023s, loss: 0.114, train accuracy: 0.953\n","Accuracy on the test set: 0.873\n","epoch: 35, time: 2397.101s, loss: 0.159, train accuracy: 0.969\n","epoch: 35, time: 2400.320s, loss: 0.081, train accuracy: 0.984\n","epoch: 35, time: 2403.545s, loss: 0.082, train accuracy: 0.977\n","epoch: 35, time: 2406.775s, loss: 0.186, train accuracy: 0.945\n","epoch: 35, time: 2409.985s, loss: 0.055, train accuracy: 0.992\n","epoch: 35, time: 2413.192s, loss: 0.120, train accuracy: 0.961\n","epoch: 35, time: 2416.441s, loss: 0.075, train accuracy: 0.969\n","epoch: 35, time: 2419.672s, loss: 0.129, train accuracy: 0.961\n","epoch: 35, time: 2422.896s, loss: 0.133, train accuracy: 0.961\n","epoch: 35, time: 2426.133s, loss: 0.093, train accuracy: 0.961\n","epoch: 35, time: 2429.359s, loss: 0.166, train accuracy: 0.922\n","epoch: 35, time: 2432.576s, loss: 0.095, train accuracy: 0.961\n","epoch: 35, time: 2435.783s, loss: 0.072, train accuracy: 0.969\n","epoch: 35, time: 2439.021s, loss: 0.073, train accuracy: 0.984\n","epoch: 35, time: 2442.245s, loss: 0.139, train accuracy: 0.945\n","epoch: 35, time: 2445.476s, loss: 0.145, train accuracy: 0.945\n","epoch: 35, time: 2448.717s, loss: 0.086, train accuracy: 0.961\n","epoch: 35, time: 2451.940s, loss: 0.216, train accuracy: 0.953\n","epoch: 35, time: 2455.172s, loss: 0.090, train accuracy: 0.969\n","epoch: 35, time: 2458.382s, loss: 0.222, train accuracy: 0.930\n","Accuracy on the test set: 0.879\n","epoch: 36, time: 2465.430s, loss: 0.060, train accuracy: 0.984\n","epoch: 36, time: 2468.679s, loss: 0.086, train accuracy: 0.969\n","epoch: 36, time: 2471.904s, loss: 0.140, train accuracy: 0.938\n","epoch: 36, time: 2475.122s, loss: 0.110, train accuracy: 0.969\n","epoch: 36, time: 2478.364s, loss: 0.071, train accuracy: 0.977\n","epoch: 36, time: 2481.589s, loss: 0.084, train accuracy: 0.969\n","epoch: 36, time: 2484.802s, loss: 0.114, train accuracy: 0.953\n","epoch: 36, time: 2488.022s, loss: 0.095, train accuracy: 0.961\n","epoch: 36, time: 2491.247s, loss: 0.117, train accuracy: 0.945\n","epoch: 36, time: 2494.465s, loss: 0.135, train accuracy: 0.930\n","epoch: 36, time: 2497.685s, loss: 0.077, train accuracy: 0.969\n","epoch: 36, time: 2500.916s, loss: 0.089, train accuracy: 0.977\n","epoch: 36, time: 2504.146s, loss: 0.082, train accuracy: 0.969\n","epoch: 36, time: 2507.396s, loss: 0.139, train accuracy: 0.938\n","epoch: 36, time: 2510.642s, loss: 0.094, train accuracy: 0.969\n","epoch: 36, time: 2513.888s, loss: 0.120, train accuracy: 0.945\n","epoch: 36, time: 2517.110s, loss: 0.100, train accuracy: 0.977\n","epoch: 36, time: 2520.327s, loss: 0.191, train accuracy: 0.945\n","epoch: 36, time: 2523.553s, loss: 0.056, train accuracy: 0.969\n","epoch: 36, time: 2526.794s, loss: 0.067, train accuracy: 0.969\n","Accuracy on the test set: 0.880\n","epoch: 37, time: 2533.900s, loss: 0.111, train accuracy: 0.961\n","epoch: 37, time: 2537.116s, loss: 0.049, train accuracy: 0.977\n","epoch: 37, time: 2540.351s, loss: 0.106, train accuracy: 0.969\n","epoch: 37, time: 2543.581s, loss: 0.106, train accuracy: 0.953\n","epoch: 37, time: 2546.817s, loss: 0.099, train accuracy: 0.961\n","epoch: 37, time: 2550.014s, loss: 0.122, train accuracy: 0.953\n","epoch: 37, time: 2553.225s, loss: 0.106, train accuracy: 0.969\n","epoch: 37, time: 2556.449s, loss: 0.113, train accuracy: 0.961\n","epoch: 37, time: 2559.692s, loss: 0.145, train accuracy: 0.953\n","epoch: 37, time: 2562.926s, loss: 0.125, train accuracy: 0.953\n","epoch: 37, time: 2566.170s, loss: 0.047, train accuracy: 0.992\n","epoch: 37, time: 2569.386s, loss: 0.127, train accuracy: 0.953\n","epoch: 37, time: 2572.614s, loss: 0.066, train accuracy: 0.961\n","epoch: 37, time: 2575.835s, loss: 0.049, train accuracy: 0.984\n","epoch: 37, time: 2579.054s, loss: 0.071, train accuracy: 0.977\n","epoch: 37, time: 2582.282s, loss: 0.064, train accuracy: 0.977\n","epoch: 37, time: 2585.512s, loss: 0.171, train accuracy: 0.930\n","epoch: 37, time: 2588.752s, loss: 0.112, train accuracy: 0.961\n","epoch: 37, time: 2591.971s, loss: 0.087, train accuracy: 0.969\n","epoch: 37, time: 2595.204s, loss: 0.081, train accuracy: 0.977\n","Accuracy on the test set: 0.884\n","epoch: 38, time: 2602.291s, loss: 0.070, train accuracy: 0.969\n","epoch: 38, time: 2605.515s, loss: 0.083, train accuracy: 0.969\n","epoch: 38, time: 2608.736s, loss: 0.037, train accuracy: 0.992\n","epoch: 38, time: 2611.945s, loss: 0.099, train accuracy: 0.961\n","epoch: 38, time: 2615.160s, loss: 0.133, train accuracy: 0.961\n","epoch: 38, time: 2618.392s, loss: 0.045, train accuracy: 0.984\n","epoch: 38, time: 2621.625s, loss: 0.119, train accuracy: 0.961\n","epoch: 38, time: 2624.856s, loss: 0.133, train accuracy: 0.961\n","epoch: 38, time: 2628.080s, loss: 0.100, train accuracy: 0.953\n","epoch: 38, time: 2631.319s, loss: 0.088, train accuracy: 0.984\n","epoch: 38, time: 2634.568s, loss: 0.019, train accuracy: 1.000\n","epoch: 38, time: 2637.792s, loss: 0.095, train accuracy: 0.977\n","epoch: 38, time: 2641.005s, loss: 0.140, train accuracy: 0.945\n","epoch: 38, time: 2644.216s, loss: 0.103, train accuracy: 0.984\n","epoch: 38, time: 2647.425s, loss: 0.156, train accuracy: 0.938\n","epoch: 38, time: 2650.655s, loss: 0.212, train accuracy: 0.945\n","epoch: 38, time: 2653.885s, loss: 0.117, train accuracy: 0.969\n","epoch: 38, time: 2657.113s, loss: 0.174, train accuracy: 0.961\n","epoch: 38, time: 2660.341s, loss: 0.032, train accuracy: 0.992\n","epoch: 38, time: 2663.582s, loss: 0.153, train accuracy: 0.953\n","Accuracy on the test set: 0.884\n","epoch: 39, time: 2670.640s, loss: 0.102, train accuracy: 0.961\n","epoch: 39, time: 2673.849s, loss: 0.035, train accuracy: 1.000\n","epoch: 39, time: 2677.074s, loss: 0.030, train accuracy: 0.992\n","epoch: 39, time: 2680.287s, loss: 0.108, train accuracy: 0.961\n","epoch: 39, time: 2683.492s, loss: 0.033, train accuracy: 0.992\n","epoch: 39, time: 2686.734s, loss: 0.070, train accuracy: 0.969\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0jcJkRe8T7BW","colab_type":"code","colab":{}},"source":["correct_total = 0\n","\n","for i, (x_batch, y_batch) in enumerate(testloader):\n","  x_batch, y_batch = x_batch.to(device), y_batch.to(device)  # Move the data to the device that is used\n","\n","  y_pred = net(x_batch)\n","  y_pred_max = torch.argmax(y_pred, dim=1)\n","\n","  correct_total += torch.sum(torch.eq(y_pred_max, y_batch)).item()\n","\n","print(f'Accuracy on the test set: {correct_total / len(testset):.3f}')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"V4RSHW_QT6Ls","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rmaaXUzTT6Ix","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0cbLSRfqT6F3","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}