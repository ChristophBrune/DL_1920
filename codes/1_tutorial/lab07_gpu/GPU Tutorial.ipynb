{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OL5Pt0sgKcKP",
    "colab_type": "text"
   },
   "source": [
    "# PyTorch - How to use the GPU\n",
    "Most of the computations that are done when training your deep learning model will consist of matrix multiplications. GPUs are optimized for these type of computations and can therefore greatly decrease the time it takes to train your model. When your models become larger, this can save hours or even days of training. This lab explains how to use the GPU in PyTorch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MHLYobqxNVit",
    "colab_type": "text"
   },
   "source": [
    "We recommend you use the GPU provided by Colab instead of the one in your laptop, as these are much better and also simplify the tool installation. Also, PyTorch makes use of CUDA, which is a platform for general purpose computing on GPUs. Not all GPUs are compatible with CUDA. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "82PYF9HWRqx-",
    "colab_type": "text"
   },
   "source": [
    "PyTorch tensors are either allocated on the CPU or GPU. Tensors located on the CPU cannot interact with those on the GPU and vice versa. This is something you need to keep track of when programming your models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "5uy0bptrIvQu",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dDINCK06uyuR",
    "colab_type": "text"
   },
   "source": [
    "If you are working in Colab, try enabling the GPU by, in the menu above, selecting 'Runtime' and 'Change runtime type'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "cnnUrjaEIvTe",
    "colab_type": "code",
    "outputId": "1f7cb668-142a-4caa-f4aa-edabf7a6373c",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.573546259141E12,
     "user_tz": -60.0,
     "elapsed": 1106.0,
     "user": {
      "displayName": "Ron van Bree",
      "photoUrl": "",
      "userId": "10574495138637938052"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available!\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "  print('GPU is available!')\n",
    "  device = torch.device('cuda')\n",
    "else:\n",
    "  print('GPU is not available!')\n",
    "  device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gmMBMJDOv3oc",
    "colab_type": "text"
   },
   "source": [
    "Next, we allocate some tensors on both the CPU and GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "cL_N6UEYIx7q",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "tensor_1 = torch.randn(4, device=torch.device('cpu'))   # CPU tensor\n",
    "tensor_2 = torch.randn(4, device=torch.device('cuda'))  # GPU tensor\n",
    "tensor_3 = torch.randn(4, device=device)                # Initialized on the device being used\n",
    "tensor_4 = torch.randn(4)                               # By default, tensors are initialized as CPU tensors\n",
    "tensor_5 = torch.FloatTensor([1,2,3,4])                 # CPU tensor\n",
    "tensor_6 = torch.cuda.FloatTensor([1,2,3,4])            # GPU tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l4CyTvazqvW6",
    "colab_type": "text"
   },
   "source": [
    "Since the following tensors are not allocated on the same device, they cannot be multiplied:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "MEX2WklWIx5C",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "tensor_1 * tensor_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BQABtpIuGiXt",
    "colab_type": "text"
   },
   "source": [
    "The .to() function can be used to move a tensor from one device to another. However, the usage of this function should be minimized, as it is a relatively expensive operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "EG0EJ6PWIvOU",
    "colab_type": "code",
    "outputId": "dc971e85-53cb-4611-a2c2-c2b18832d972",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.573546281031E12,
     "user_tz": -60.0,
     "elapsed": 810.0,
     "user": {
      "displayName": "Ron van Bree",
      "photoUrl": "",
      "userId": "10574495138637938052"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34.0
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3309,  3.1425,  0.6378,  0.0419], device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_3 * tensor_4.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kbm3qRw7r4t0",
    "colab_type": "text"
   },
   "source": [
    "Since model parameters are tensors, they are allocated on either the CPU or GPU as well. Therefore, models allocated on some device cannot process data on another device. Using the .to() function it is possible to move entire models between devices as well (if they extend torch.nn.Module)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "kcT6OvS7H7Jc",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "from torch.nn import Module, Parameter\n",
    "\n",
    "# Define a small Module example to move between devices\n",
    "class Neuron(Module):\n",
    "\n",
    "  def __init__(self):\n",
    "    super(Neuron, self).__init__()\n",
    "    self.weights = Parameter(torch.randn(4))\n",
    "    self.bias = Parameter(torch.randn(1))\n",
    "  \n",
    "  def forward(self, x):\n",
    "    return torch.sigmoid(torch.sum(self.weights * x) + self.bias)\n",
    "\n",
    "\n",
    "neuron = Neuron().to(device)\n",
    "neuron.forward(tensor_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eLRDKibr0TCQ",
    "colab_type": "text"
   },
   "source": [
    "### Exercise\n",
    "Try modifying the code below such that the training loop uses the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "Y538VmgqH7PP",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "DumY48me1lFM",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# This code does not need to be modified!\n",
    "\n",
    "class TwoLayerNet(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size1, output_size):\n",
    "        super(TwoLayerNet , self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Linear(input_size, hidden_size1, bias=False)\n",
    "        self.layer2 = nn.Linear(hidden_size1, output_size, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y       = self.layer1(x)\n",
    "        y_hat   = F.relu(y)\n",
    "        z       = self.layer2(y_hat)\n",
    "        return F.softmax(z, dim=1)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "4ZOm33Ek0sMz",
    "colab_type": "code",
    "outputId": "f8250875-252b-4f80-b2b3-dfc745319052",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.573549819212E12,
     "user_tz": -60.0,
     "elapsed": 7115.0,
     "user": {
      "displayName": "Ron van Bree",
      "photoUrl": "",
      "userId": "10574495138637938052"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255.0
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data_mnist/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9920512it [00:02, 3869971.49it/s]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data_mnist/MNIST/raw/train-images-idx3-ubyte.gz to ./data_mnist/MNIST/raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data_mnist/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32768it [00:00, 57378.86it/s]                           \n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data_mnist/MNIST/raw/train-labels-idx1-ubyte.gz to ./data_mnist/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data_mnist/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1654784it [00:01, 955833.47it/s]                             \n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data_mnist/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data_mnist/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data_mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8192it [00:00, 21722.03it/s]            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data_mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data_mnist/MNIST/raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# This code does not need to be modified!\n",
    "\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Lambda(lambda x: x.squeeze()),\n",
    "                                ])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data_mnist',\n",
    "                                      train=True,\n",
    "                                      download=True,\n",
    "                                      transform=transform\n",
    "                                      )\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data_mnist',\n",
    "                                     train=False,\n",
    "                                     download=True,\n",
    "                                     transform=transform\n",
    "                                     )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "zdfdpqBT0sKH",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# This code does not need to be modified!\n",
    "\n",
    "bs=128\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset,\n",
    "                                          batch_size=bs,\n",
    "                                          shuffle=True,\n",
    "                                          drop_last=True\n",
    "                                          )\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset,\n",
    "                                         batch_size=bs,\n",
    "                                         shuffle=True,\n",
    "                                         drop_last=True\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "nTRu0f-m0sD9",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# Modify the following code such that it uses the GPU\n",
    "\n",
    "net=TwoLayerNet(784, 64, 10)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.SGD(net.parameters() , lr=0.1)\n",
    "\n",
    "for epoch in range(1, 5):\n",
    "\n",
    "  for i, (minibatch_data, minibatch_label) in enumerate(trainloader):\n",
    "\n",
    "      # Set dL/dU, dL/dV, dL/dW to be filled with zeros\n",
    "      optimizer.zero_grad()\n",
    "      \n",
    "      #reshape the minibatch\n",
    "      inputs = minibatch_data.view(bs, 784)\n",
    "\n",
    "      # forward the minibatch through the net  \n",
    "      prob=net(inputs) \n",
    "      \n",
    "      # Compute the average of the losses of the data points in the minibatch\n",
    "      loss = criterion(prob , minibatch_label) \n",
    "      \n",
    "      # backward pass to compute dL/dU, dL/dV and dL/dW    \n",
    "      loss.backward()\n",
    "      \n",
    "      # do one step of stochastic gradient descent: U=U-lr(dL/dU), V=V-lr(dL/dU), ...\n",
    "      optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "c4zHiewpH7Mt",
    "colab_type": "code",
    "outputId": "70f0820b-0e99-4ad7-d138-aae03fed8031",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.573550471372E12,
     "user_tz": -60.0,
     "elapsed": 1016.0,
     "user": {
      "displayName": "Ron van Bree",
      "photoUrl": "",
      "userId": "10574495138637938052"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 486.0
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence scores:\n",
      "0: 0.008735028095543385\n",
      "1: 2.560830944275949e-05\n",
      "2: 0.0009293326875194907\n",
      "3: 0.014074495062232018\n",
      "4: 0.0034834493417292833\n",
      "5: 0.7097895741462708\n",
      "6: 0.0015291203744709492\n",
      "7: 9.452593076275662e-06\n",
      "8: 0.24014447629451752\n",
      "9: 0.021279430016875267\n",
      "\n",
      "Label with highest confidence score: 5\n"
     ]
    }
   ],
   "source": [
    "# This code does not need to be modified!\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# choose a picture at random\n",
    "im_minibatch, label_minibatch = iter(testloader).next()\n",
    "im, label = im_minibatch[0].cpu(), label_minibatch[0].cpu()\n",
    "\n",
    "# Function to show an image tensor\n",
    "def show(X):\n",
    "    if X.dim() == 3 and X.size(2) == 3:\n",
    "        plt.imshow(X.numpy())\n",
    "        plt.show()\n",
    "    elif X.dim() == 2:\n",
    "        plt.imshow(   X.numpy() , cmap='gray'  )\n",
    "        plt.show()\n",
    "    else:\n",
    "        print('WRONG TENSOR SIZE')\n",
    "\n",
    "# diplay the picture\n",
    "show(im)\n",
    "\n",
    "# feed it to the net and display the confidence scores\n",
    "prob = net.cpu()(im.view(1,784)) \n",
    "\n",
    "print('Confidence scores:\\n' + '\\n'.join(['{}: {}'.format(i, p.item()) for i, p in enumerate(prob.squeeze())]))\n",
    "\n",
    "print('\\nLabel with highest confidence score: {}'.format(torch.argmax(prob).item()))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "GPU Tutorial.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
